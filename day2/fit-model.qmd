---
title: "Fitting the model"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width=7, fig.height=5, fig.retina=3,
  out.width = "80%", fig.align = "center",
  cache = FALSE,
  echo = FALSE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)

library(tidyverse)
library(patchwork)
library(brms)
library(bayesplot)

options(dplyr.summarise.inform = FALSE)
theme_set(theme_bw())

acc <- read_csv('../data/acceptance.csv') %>% 
    mutate(
    cond = ifelse(condition == 'Comprehension', -0.5, 0.5),
    sent = ifelse(sentence_type == 'case_marking', -0.5, 0.5),
    condsent = cond * sent * 2
  )
```


On this page, we'll focus on:

1. **Fitting the model** (yay!)
2. **A couple diagnostics** for checking model convergence.
3. **Posterior predictive checks:** How well does the model capture the generative process behind the data?

Then on the next page, we'll use what we learned yesterday about how to interpret posterior probabilities to finally interpret and report the model's results.


## Our model

In mathematical notation, the model we'll fit is the following.

$$
\begin{aligned}
\text{acc} & \sim Bernoulli(\theta) \\
logit(\theta) & = \alpha + (\beta_1 \cdot cond) + (\beta_2 \cdot sent) + (\beta_3 \cdot cond \cdot sent)\\
\alpha & \sim Normal(0, 1.5) \\
\beta_1 & \sim Normal(0, 1) \\
\beta_2 & \sim Normal(0, 1) \\
\beta_3 & \sim Normal(0, 1) \\
\end{aligned}
$$


In brms notation, it looks like this:

```{r eval=FALSE, echo=TRUE}
acc_fit <- brm(sentence_accepted ~ cond + sent + condsent,
               data = acc,
               family = bernoulli(),
               prior = c(
                 prior(normal(0, 1.5), class = Intercept),
                 prior(normal(0, 1), class = b)
               ),
               backend = 'cmdstanr'
)
```


Copy this code into your notebook and run it to finally fit our model 🥳

```{r}
acc_fit <- brm(sentence_accepted ~ cond + sent + condsent,
               data = acc,
               family = bernoulli(),
               prior = c(
                 prior(normal(0, 1.5), class = Intercept),
                 prior(normal(0, 1), class = b)
               ),
               backend = 'cmdstanr',
               file = '../data/models/acc_fit'
)

summary(acc_fit)
```



# Checking convergence

Recall from yesterday that the Markov Chain Monte Carlo chains that sample from the posterior are skating around the posterior landscape, trying to find where the posterior probability mass is.
It's possible that they never find it, or might not do so reliably.
When the chains don't correctly sample from the posterior, we say the model has not converged.

We want the chains to "mix": in other words, all four chains should be sampling from the same posterior.
There are a couple ways to tell that this is working as it should.


## "Fat hairy caterpillars"

Run the following code to produce so-called "trace plots".

```{r trace-plots, echo=TRUE}
mcmc_trace(acc_fit,
           pars = c('b_Intercept', 'b_cond', 'b_sent', 'b_condsent'))
```

Trace plots show the traces of the four chains as they traversed the posterior during sampling.
The y axis represents the values of the posterior, and the x axis shows the sample indices (each chain drew 1,000 samples).

If the chains mixed well, then in the words of Shravan Vasishth, the trace plots should look like "fat hairy caterpillars".
And these ones do!

For comparison, here's an example of some that don't.

```{r bad-trace}
badfit <- brm(sentence_accepted ~ cond + sent + condsent,
               data = acc,
               family = bernoulli(),
               prior = c(
                 prior(normal(0, 1.5), class = Intercept),
                 prior(normal(0, 100), class = b)
               ),
               backend = 'cmdstanr',
              iter = 1000,
              warmup = 10,
               file = '../data/models/badfit'
)

mcmc_trace(badfit,
           pars = c('b_Intercept', 'b_cond', 'b_sent', 'b_condsent'))
```

These still mix a little, but they don't overlap quite as densely as the previous ones.
In even worse scenarios, you might also sometimes see chains that get stuck in one part of the posterior and never mingle with the others.
But as long as your trace plots look like fat hairy caterpillars, then that's a good first sign 🐛


## Rhat = 1.00

The second thing to check is more quantitative.
For each parameter, the model gives us a diagnostic measure called Rhat ([ɑɹ hæt]).
Rhat is a measure of how well the chains have mixed.
It compares the estimates within chains and between chains, and if the chains have mixed well, then the result should be near 1.
We consider the model to have converged if all Rhat values are equal to 1.00; even an Rhat of 1.01 should make you suspicious.

If we display the summary of our model, then there will be a column called Rhat; all values should be equal to 1.00.
(We'll look at the other values soon!)

```{r}
summary(acc_fit)
```

And so they are!

If there is any Rhat value greater than 1.00, then it means the chains didn't mix well enough, and we cannot trust the posterior estimates we've been given.
For reference, the model that yielded the bad trace plot above had Rhats = 1.04.

**If you do ever fit a model that yields Rhat > 1.00, [this page](https://mc-stan.org/misc/warnings.html) offers some solutions you could try.**

OK: our model `acc_fit` has converged.
But there's one more thing to check before we can trust its estimates: **does it do a good job of generating data that looks like the data we observed?**
One way to find out...


# Posterior predictive checks

We've encountered the concept of predictive checks before.

- With *prior* predictive checks, we drew sample values from each parameter's *prior*, used those values to define the likelihood, and used that likelihood to generate simulated outcomes.
- Now, with *posterior* predictive checks, we draw sample values from each parameter's *posterior*, and use them to do the exact same thing.

In fact, the processes are so similar that we can use `pp_check()` again to give us the means of the posterior predictive distributions, and compare them to the mean of the observed data:

```{r pstr-pred, message=FALSE}
pp_check(acc_fit, type = 'stat', stat = mean)
```

The mean of our observed data (the dark vertical line) is smack in the middle of the posterior predictive distribution of means—hurray!
That's what we want to see.
If the observed data were far away from the bulk of the posterior predictive distribution, then that would suggest that we might want to change the model architecture to better reflect the generative process behind the data.

Reassured that the model does a decent job of capturing the data, we continue to the Grand Finale: interpreting and reporting the model's results.

