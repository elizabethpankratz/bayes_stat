[
  {
    "objectID": "day1/cri.html",
    "href": "day1/cri.html",
    "title": "95% Credible Intervals",
    "section": "",
    "text": "The 95% Credible Interval (abbreviated as 95% CrI) defines the range in which the model is 95% certain that the true value lies.\nThe 95% CrI has this interpretation because it contains 95% of the probability mass of the entire posterior distribution. And not just any 95%: it contains the highest-density 95% (i.e., the ‚Äútallest‚Äù and ‚Äúnarrowest‚Äù region possible).\nPut differently, the 95% CrI spans the distribution from the 2.5th quantile up to the 97.5th quantile.\nHere‚Äôs how it looks, shaded in light blue:\n\n\n\n\n\nThe interpretation of the Bayesian Credible Interval‚Äîthe range within which, with 95% probability, the true value lies‚Äîis the interpretation we often instinctively want the frequentist confidence interval to have.\nThis is considered by some to be a point in favour of the Bayesian framework: it reflects our intuitions a little bit better."
  },
  {
    "objectID": "day1/dancing-pstrs.html",
    "href": "day1/dancing-pstrs.html",
    "title": "Dancing posteriors (üòè)",
    "section": "",
    "text": "Recall that, across all 100 observations,\n\\[\n\\text{correct} = \\frac{80}{100}\n\\]\n\nDo the posteriors eventually approach this proportion?\nHow does the updating process differ when we start with different priors?\n\nLet‚Äôs have a look:"
  },
  {
    "objectID": "day1/day1-recap.html",
    "href": "day1/day1-recap.html",
    "title": "Day 1 Recap",
    "section": "",
    "text": "How are beliefs about the world formalised?\n\nAs probability distributions over the possible values that the parameter in question could take on.\n\nHow do different priors influence a model‚Äôs estimates?\n\nModels with uninformative priors learn more from the data than do models with informative priors.\nBut as the amount of data grows, the influence of the prior wanes.\n\nHow do we interpret a posterior probability distribution?\n\nIt tells us how probable each possible parameter value is, given the data and the priors.\nThe 95% Credible Interval gives us the range in which the model is 95% certain that the true value lies.\n\nHow does the model find the posterior distribution?\n\nbrms models use an algorithm called Markov Chain Monte Carlo to approximate the posterior by drawing many samples from it.\n\nHow do Bayesian models generate data?\n\nThe likelihood‚Äôs parameter values can be sampled from the prior distributions.\nAnd then we sample from the likelihood to generate one data point."
  },
  {
    "objectID": "day1/example.html",
    "href": "day1/example.html",
    "title": "Example: Lexical decision task data",
    "section": "",
    "text": "Imagine that we‚Äôve run a lexical decision experiment. Participants are shown a word, and they have to decide whether it is a real word or not.\nWe have data from 100 trials. Out of those 100 trials, participants answered correctly 80 times.\n\\[\n\\text{correct} = \\frac{80}{100}\n\\] \nWhat is the probability of answering correctly?"
  },
  {
    "objectID": "day1/generative-models.html",
    "href": "day1/generative-models.html",
    "title": "Bayesian models as generative models",
    "section": "",
    "text": "Bayesian models can be thought of as models of the generative process that produced the data.\nIn a way, what we‚Äôve been doing so far is playing the model ‚Äúbackward‚Äù: we‚Äôre using observed data to try to figure out plausible values of the parameters inside the model.\nBut we can also play the model ‚Äúforward‚Äù, and use the parameter values inside the model to generate data that the model thinks is plausible.\nWe‚Äôll see tomorrow why this can be a useful thing to do!\nHere, we‚Äôll see how it works.\n\nImagine our model is:\n\\[\n\\begin{aligned}\n\\text{data} &\\sim Normal(\\mu, \\sigma) \\\\\n\\mu &\\sim Normal(0, 1.5)\\\\\n\\sigma &\\sim Uniform(0, 5)\\\\\n\\end{aligned}\n\\]\nThe following procedure will let us use this model to generate one data point.\n\nTo generate data from the likelihood \\(Normal(\\mu, \\sigma)\\), we need to define the currently-unknown parameters.\n\nFor \\(\\mu\\), sample one value \\(a\\) from \\(Normal(0, 1.5)\\).\nFor \\(\\sigma\\), sample one value \\(b\\) from \\(Uniform(0, 5)\\).\n\nCombine them to define the likelihood as \\(Normal(a, b)\\).\nSample one value from this distribution: this is one observation.\n\n\nAnd to generate a whole dataset of size \\(n\\), we would just repeat this procedure \\(n\\) times. The distribution of data resulting from this procedure is sometimes called a ‚Äúpredictive distribution‚Äù.\nHere‚Äôs some R code that implements this procedure and plots the resulting predictive distribution.\n\n# Set seed for reproducibility.\nset.seed(1)\n\n# Set the number of iterations.\nn <- 10000\n\n# Define an accumulator list that will contain \n# each iteration's data point.\naccum <- c()\n\n# Let's gooooooo\nfor(i in 1:n){\n  a     <- rnorm(1, mean = 0, sd = 1.5)\n  b     <- runif(1, min = 0, max = 5)\n  data  <- rnorm(1, mean = a, sd = b)\n  accum <- c(accum, data)\n  \n  ## Comment in to print out each iteration's likelihood\n  ## (best make n smaller first!)\n  # print(paste0('Sampled ', round(data, 2), ' from Normal(', round(a, 2), ', ', round(b, 2), ')'))\n}\n\n# Make a density plot.\ntibble(data = accum) %>% \n  ggplot(aes(x = data)) +\n  geom_density(fill = 'grey', alpha = .5)\n\n\n\n\nSo, the model\n\\[\n\\begin{aligned}\n\\text{data} &\\sim Normal(\\mu, \\sigma) \\\\\n\\mu &\\sim Normal(0, 1.5)\\\\\n\\sigma &\\sim Uniform(0, 5)\\\\\n\\end{aligned}\n\\]\nconsiders data in the range of about [‚Äì10, 10] to be plausible outcomes.\nWhether or not we agree will depend on the data and our real-world knowledge about it!\nTomorrow we‚Äôll see how generating predictive distributions fits into the Bayesian modelling workflow.\n\n\n\n\n\n\nTry it yourself!\n\n\n\n\n\nThe example here was for a model with a Normal likelihood that contains two parameters, one for the mean and the other for the standard deviation.\nCan you adapt this code for one or both of the Bernoulli models we‚Äôve been using for the lexical decision data?\nThe procedure will be similar:\n\nSample a value \\(t\\) from the prior for theta.\n\nrbeta(1, x, y) for \\(Beta(x, y)\\) will be your friend here.\n\nUse \\(t\\) to define a Bernoulli likelihood, and from that, sample one simulated observation.\n\nrbinom(1, 1, t) will produce a 1 with probability \\(t\\) and a 0 with probability \\(1-t\\).\n\n\nBinary data like this results in funny bimodal density plots, but still, give it a shot.\nAnd if you feel daring, you could simulate many datasets, get the mean amount of sentences accepted in each (i.e., the proportion of responses = 1), and plot a histogram of the means of all those individual predictive distributions. That‚Äôs the kind of plot we‚Äôll be looking at tomorrow."
  },
  {
    "objectID": "day1/informative-prior.html",
    "href": "day1/informative-prior.html",
    "title": "An informative prior",
    "section": "",
    "text": "We might also have entered the experiment with a belief (for whatever reason) that participants will perform around chance.\nBefore uncollapsing this box, think about what a prior that reflects this belief might look like.\n\n\n\n\n\n\nA prior for success around chance\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we observe the first trial, a success.\nThink again about how you expect the posterior to look before uncollapsing this next box.\n\n\n\n\n\n\nAfter observing one successful trial\n\n\n\n\n\n\n\n\n\n\nDid the posterior (dark blue) move as far from the prior (light blue) as you expected?\n\n\n\nNow let‚Äôs watch data from the first ten trials come in.\n\n\n\n\n\n\nThe first ten trials\n\n\n\n\n\n\n\n\n\nHow does this differ from the situation before, where we began with a uniform prior?"
  },
  {
    "objectID": "day1/interim-summary.html",
    "href": "day1/interim-summary.html",
    "title": "Interim summary: The influence of priors",
    "section": "",
    "text": "‚ÄúPriors affect the posterior so much! Isn‚Äôt that an unscientific way to do data analysis?‚Äù\n\nIf you were gonna actively mess with your analysis, using priors would be an embarrassingly transparent way to do it.\nAll analyses make assumptions, and all analyses are subjective. Bayesian approaches let you formalise your assumptions and put them up for debate.\nAlso: as the amount of data increases, the model‚Äôs reliance on the priors decreases.\n\n\nAny other questions?"
  },
  {
    "objectID": "day1/learningobj.html",
    "href": "day1/learningobj.html",
    "title": "Learning objectives for Day 1",
    "section": "",
    "text": "How are beliefs about the world formalised?\nHow do different priors influence a model‚Äôs estimates?\nHow do we interpret a posterior probability distribution?\nHow does the model find the posterior distribution?\nHow do Bayesian models generate data?"
  },
  {
    "objectID": "day1/mcmc.html",
    "href": "day1/mcmc.html",
    "title": "Finding posteriors through sampling",
    "section": "",
    "text": "Bayesian data analysis is seen as a cutting-edge, up-and-coming method, even though Bayes‚Äô Theorem dates back to 1763. Why is that? What stopped the Bayesian framework from taking off sooner?\nThe short answer: Computational limitations.\nThe slightly longer answer: Estimating posterior distributions analytically (i.e., using calculus) works sometimes, but it‚Äôs not possible for every kind of model. Another way to find the posterior is to approximate it by drawing samples from it. This is computationally challenging, and it‚Äôs only fairly recently that sampling methods have been developed so that posteriors can be estimated by any old linguist with a laptop.\n\n\n\n\n\n\nThe longest answer\n\n\n\n\n\nSee:\n\nChapter 3 (available online) of Bayesian Data Analysis for Cognitive Science (Nicenboim, Schad, Vasishth)\nChapter 9 of Statistical Rethinking (McElreath)\n\n\n\n\nThe method that brms uses is called Markov Chain Monte Carlo, or MCMC. It treats the posterior as a kind of landscape, and identifies the areas of highest probability essentially by running a physics simulation.\nImagine there‚Äôs a pit or a crater in the ground, and you throw a bouncy ball in. The ball will bounce around a lot, but most of the time, it‚Äôll bounce toward/around the deepest point in the middle of the crater.\nThis means that, if every bounce is a sample, then you‚Äôll have more samples from the part where the pit is deepest. If you take these samples and make a density plot, you‚Äôll have higher density (because you have more samples) in the area of greatest depth.\n\nThis is basically how MCMC works: by bouncing around the unknown/unknowable ‚Äúlandscape‚Äù of the posterior and exploring it by sampling, giving us more samples (= higher probability density) in those deeper (= higher-probability) areas.\nIf we take enough samples, we still get a decent approximation of the shape of the posterior, without needing to compute it exactly.\nIn reality, brms doesn‚Äôt just throw one proverbial bouncy ball into the proverbial pit, but four (and they‚Äôre also not called bouncy balls, but ‚Äúchains‚ÄùÔ∏è üòî). These chains run, by default, for 2,000 iterations each.\nThe first 1,000 iterations are used to explore the posterior landscape and hopefully eventually find a higher-probability area: this is called the ‚Äúwarm-up‚Äù or ‚Äúburn-in‚Äù period.\nThe 1,000 iterations after that are spent traversing the posterior and drawing samples. And it‚Äôs those samples that brms will summarise when it describes each parameter‚Äôs posterior to us."
  },
  {
    "objectID": "day1/notation.html",
    "href": "day1/notation.html",
    "title": "Notation",
    "section": "",
    "text": "Let‚Äôs formalise the two models we‚Äôve just seen using some standard notation.\nThe model with the uniform prior:\n\\[\n\\begin{aligned}\n\\text{correct} &\\sim Bernoulli(\\theta) \\\\\n\\theta &\\sim Beta(1, 1)\n\\end{aligned}\n\\]\nThe model with the prior around chance:\n\\[\n\\begin{aligned}\n\\text{correct} &\\sim Bernoulli(\\theta) \\\\\n\\theta &\\sim Beta(20, 20)\n\\end{aligned}\n\\]\n\nThe distribution that the data, \\(\\text{correct}\\), follows is called the likelihood.\n\nHere, it‚Äôs \\(Bernoulli(\\theta)\\).\n\nThe distribution that the model-internal parameter \\(\\theta\\) follows is called the prior.\n\nHere, it‚Äôs \\(\\theta \\sim Beta(x, y)\\).\n\n\n\n\n\n\n\n\nThe many shapes of the beta distribution\n\n\n\n\n\nHere are some examples of how beta distributions can look:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe beta-binomial conjugate case, or, why not Uniform(0, 1)?\n\n\n\n\n\nWe could just as easily have specified the first model as:\n\\[\n\\begin{aligned}\n\\text{correct} &\\sim Bernoulli(\\theta) \\\\\n\\theta &\\sim Uniform(0, 1)\n\\end{aligned}\n\\]\nBut we like to use beta priors for a binomial (Bernoulli) outcome because this is an example of a conjugate case.\nA conjugate case is a kind of model where the posterior and the prior belong to the same distribution, as long as a particular likelihood is used. Here, with a Bernoulli likelihood, both prior and posterior belong to beta distributions: the so-called ‚Äúbeta-binomial conjugate case‚Äù.\nSo, for internal consistency, I‚Äôm using beta priors throughout."
  },
  {
    "objectID": "day1/posteriors.html",
    "href": "day1/posteriors.html",
    "title": "Interpreting posteriors",
    "section": "",
    "text": "Here‚Äôs how the posterior distribution of the uniform-prior model looks after observing 80 successes out of 100 trials:\n\n\n\n\n\nThe x axis shows every possible probability for answering the lexical decision task correctly. The model allocates its belief over these probabilities based on the data we observed and the prior beliefs we encoded.\nThe posterior probability distribution therefore shows the model‚Äôs belief about how likely different probabilities are to be the true probability that generated the data.\nHere, the model considers probabilities of success between about 0.7 and 0.9 to be the most plausible, given the observed data and our uniform prior beliefs.\nWe usually report posterior distributions by summarising their central tendency and dispersion.\n\nCentral tendency: mean or median (depending on how skewed the distribution is).\nDispersion: conventionally, the 95% Credible Interval."
  },
  {
    "objectID": "day1/probable-probs.html",
    "href": "day1/probable-probs.html",
    "title": "Probable probabilities",
    "section": "",
    "text": "We have intuitions about how probable different probabilities of success are, given the data we observed.\nWe visualise them as a probability distribution:\n\n\n\n\n\nThis is a posterior probability distribution, because it takes into account information about the data, as well as (implicitly, so far) information about how probable we think each probability is to begin with.\nLet‚Äôs make that implicit information explicit and see how we arrived at this posterior distribution step by step."
  },
  {
    "objectID": "day1/unif-prior.html",
    "href": "day1/unif-prior.html",
    "title": "A uniform prior",
    "section": "",
    "text": "We might enter the experiment with no expectations about how well or how poorly participants will perform.\n\n\n\n\n\nNow, we observe one trial: a success!\nThis changes our belief about the probable probabilities of success: we know it can‚Äôt be zero, because we just saw a success.\nIn fact, here‚Äôs how our belief about every probability looks now, superimposed over our prior:\n\n\n\n\n\n\n\n\n\nLet‚Äôs let a couple more trials trickle in:\n\n\n\n\n\n\n\n\nAnd even more‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\nWhat is happening here?"
  },
  {
    "objectID": "day2/build-model.html",
    "href": "day2/build-model.html",
    "title": "Building the model",
    "section": "",
    "text": "Recall from yesterday that a Bayesian model combines the likelihood of the data, given different hypotheses, with the prior probabilities of those hypotheses, to give us the posterior probabilities of how probable different hypotheses are, given the data.\nWhen we‚Äôre doing Bayesian inference, these hypotheses correspond to different parameter values: different values that the model‚Äôs coefficients can plausibly take on. The priors over these parameter values define how plausible we think different values are a priori. And the model will produce posteriors over the same parameter values that tell us what parameter values the model thinks are plausible, given the data.\nBut before we get to the posterior, we‚Äôll need to define for our model a likelihood and some priors. We‚Äôll start with the likelihood because it‚Äôs the closest part of the model to the outcome, so it‚Äôs the best starting point for figuring out a model that could plausibly have generated the outcome we observe. And it‚Äôll also affect how our priors will look later."
  },
  {
    "objectID": "day2/build-model.html#choosing-a-likelihood",
    "href": "day2/build-model.html#choosing-a-likelihood",
    "title": "Building the model",
    "section": "Choosing a likelihood",
    "text": "Choosing a likelihood\nIf you‚Äôve done linear modelling before, you‚Äôve probably already encountered this idea under the guise of the ‚Äúmodel family‚Äù. If you know that you can fit a basic linear model to continuous outcome data, but that you need a binomial/Bernoulli/logistic model for binary outcome data, then you know how to choose a likelihood function.\nThe likelihood function is selected based on the kinds of values that the outcome variable can take on. Here are three common examples.\n\nGaussian (normal)\nA continuous variable, such as formant values in Hz, can be said to follow a Gaussian distribution (a.k.a. a normal distribution). A Gaussian distribution looks something like this:\n\n\n\n\n\n\n\n\n\nSo if we were modelling Hz, we would use a Gaussian likelihood, and we could write it like this:\n\\[\nHz \\sim Gaussian(\\mu, \\sigma)\n\\]\n‚ÄúHz is distributed according to a Gaussian distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).‚Äù\n(\\(\\mu\\) and \\(\\sigma\\) are parameters that define the distribution‚Äôs shape: where its mean is located, and how spread-out the distribution is around that mean, respectively.)\nIf you would use a basic linear model, not a generalised linear model, in your analysis, you would choose a Gaussian likelihood.\n\n\nLog-normal\nA continuous variable that‚Äôs positive-only and right-skewed, such as reaction times, might follow a Gaussian distribution only once it‚Äôs been log-transformed. This means that, without any transformation, it follows a log-normal distribution.\n\n\n\n\n\n\n\n\n\nWe write this as:\n\\[\nRT \\sim LogNormal(\\mu, \\sigma)\n\\]\n‚ÄúRT is distributed according to a lognormal distribution with location \\(\\mu\\) and scale \\(\\sigma\\).‚Äù\nEquivalently, one could log-transform RTs and model them with a Gaussian likelihood:\n\\[\n\\log(RT) \\sim Gaussian(\\mu, \\sigma)\n\\]\nThis is how it‚Äôs typically done in frequentist circles, but Bayesian models make it just as easy to use a lognormal likelihood as a Gaussian one.\n\n\nBernoulli\nIf the outcome is binary (e.g., 0/1, success/failure, grammatical/ungrammatical, English/French, etc.), then we assume that it comes from a Bernoulli distribution defined by \\(\\theta\\), the probability of success.\n\\[\nsuccess \\sim Bernoulli(\\theta)\n\\]\n‚ÄúSuccess is distributed according to a Bernoulli distribution with probability \\(\\theta\\).‚Äù\nOur model today will have a Bernoulli likelihood, since our data is binary: 0 if the participant rejected the sentence they saw, 1 if they accepted it.\n\n\n\n\n\n\nIntuition: How can a probability produce binary outcomes?\n\n\n\n\n\nImagine a flip of a fair coin, where the probability of getting heads is \\(\\theta = 0.5\\). In Bernoulli terms, the outcome representing ‚Äúsuccess‚Äù is generated with this probability \\(\\theta\\), and the outcome representing ‚Äúfailure‚Äù is generated with probability \\(1 - \\theta\\).\nSo, if you flip a fair coin ten times, you‚Äôll get ten observations of binary outcomes (heads/tails), and probably about half of them will be heads (‚Äúsuccess‚Äù).\n\n\n\n\n\nOther likelihoods\nOther likelihoods you may encounter include Poisson (for count data) or beta (for data in [0, 1]), and there are more besides. But for the usual analyses in experimental linguistics, the three above will be ones we reach for."
  },
  {
    "objectID": "day2/build-model.html#begin-building-the-model",
    "href": "day2/build-model.html#begin-building-the-model",
    "title": "Building the model",
    "section": "Begin building the model",
    "text": "Begin building the model\nNow that we have a likelihood, the next step is to think about what priors our model needs.\nYou saw that each of those likelihood functions above contains parameters that define their shape: \\(\\mu\\), \\(\\sigma\\), \\(\\theta\\). Every parameter in a Bayesian model needs to have a prior that tells the model which values are a priori plausible for that parameter to take on.\nIn this section, we build the model up bit by bit. This process will show us how many parameters our model has, and therefore what priors the model needs. We‚Äôll start with the likelhood.\nTo build up the model, we‚Äôll start off by defining the model‚Äôs likelihood.\nWe determined which likelihood we need above: acceptance \\(acc\\) follows a Bernoulli distribution.\n\\[acc \\sim Bernoulli(\\theta)\\]\nWe‚Äôre interested in modelling what affects \\(\\theta\\), the probability of accepting a sentence. In other words, we want \\(\\theta\\) to be able to take on different values, depending on the condition participants were in and what kinds of sentences they were seeing. \\(\\theta\\) should be high in a certain situation if participants are more likely to accept sentences, and it should be low in another situation if they are more likely to reject them.\nOK, well, more precisely: we actually want \\(logit(\\theta)\\), the log-odds of accepting a sentence, to be able to take on different values. Converting probabilities (bounded between 0 and 1) into log-odds (unbounded) moves our estimation into a continuous space in which a line, also a continuous thing, can reasonably be fit.\n\n\n\n\n\n\nWhat are log-odds and why do we use them?\n\n\n\n\n\nYou‚Äôve probably heard sayings like ‚ÄúWhat are the odds?‚Äù\nWe use the term ‚Äúodds‚Äù colloquially, but it also has a formal definition in statistics:\n\\[\n\\text{odds} = \\frac{\\text{probability of a thing happening}}{\\text{probability of the thing not happening}} = \\frac{p}{1-p}\n\\] Odds can be > 1 (if \\(p\\) > 0.5) or < 1 (if \\(p\\) < 0.5), but they cannot be negative. However, we want a scale that can take on positive or negative values, giving us a continuous space to fit a line in.\nIf we take the log of the odds, this effectively ‚Äúunsquishes‚Äù the region in [0, 1], mapping it to (\\(-\\infty\\), 0].\nSo, log-odds are a transformation of probabilities from [0, 1] into the range (\\(-\\infty\\), \\(\\infty\\)).\n\n\n\n\n\n\n\n\n\n\n\n\nIn a linear modelling approach, we allow \\(logit(\\theta)\\) to take on different values depending on the values of our predictors (\\(cond\\) for condition, \\(sent\\) for sentence) and their interaction (\\(cond \\cdot sent\\)) by setting it equal to this linear expression:\n\\[\nlogit(\\theta) = \\alpha + (\\beta_1 \\cdot cond) + (\\beta_2 \\cdot sent) + (\\beta_3 \\cdot cond \\cdot sent)\n\\]\nThere‚Äôs a lot of Greek here! \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) are the parameters that we want our model to estimate. You might recognise \\(\\alpha\\) as the line‚Äôs intercept, and all the \\(\\beta\\)s are the slopes, aka the effects, aka the coefficients of our predictors. We will need priors for all of these parameters.\nThe priors will tell the model which values for each parameter we think are plausible, and they do this by describing how these parameter values are distributed. So, formally(ish), we‚Äôll end up with a model that looks like this:\n\\[\n\\begin{aligned}\n\\text{acc} & \\sim Bernoulli(\\theta) \\\\\nlogit(\\theta) & = \\alpha + (\\beta_1 \\cdot cond) + (\\beta_2 \\cdot sent) + (\\beta_3 \\cdot cond \\cdot sent)\\\\\n\\alpha & \\sim \\text{something} \\\\\n\\beta_1 & \\sim \\text{something} \\\\\n\\beta_2 & \\sim \\text{something} \\\\\n\\beta_3 & \\sim \\text{something} \\\\\n\\end{aligned}\n\\]\nFinding suitable ‚Äúsomething‚Äùs is the focus of the next section."
  },
  {
    "objectID": "day2/build-model.html#choosing-priors",
    "href": "day2/build-model.html#choosing-priors",
    "title": "Building the model",
    "section": "Choosing priors",
    "text": "Choosing priors\nThere is a lot of literature on how to choose appropriate priors, and many different schools of thought.\nDifferent kinds of priors you might encounter:\n\nInformative priors: Priors that are quite narrow, reflecting more a priori certainty about the values that we believe to be plausible. These priors might come from, e.g.:\n\nDomain knowledge elicited from experts.\nEffect size estimates from one‚Äôs own previous research or from meta-analyses.\n\nWeakly regularising priors: Priors that are fairly broad, reflecting a priori uncertainty about plausible parameter values. They rule out impossibly large values (e.g., RTs in the millions), but are uncertain enough that the data has much more of an influence on the posterior than they do.\nbrms‚Äô default priors: If you don‚Äôt specify any priors when you fit your model, brms will use its defaults. (You can see what these are with the command brms::get_priors(mymodel).)\n\nI personally prefer to use weakly regularising priors, because (a) there often isn‚Äôt lots of domain knowledge available for the kinds of studies I‚Äôm running, and (b) weaker priors are less philosophically alarming to researchers trained in frequentism, and these are generally the people who review our papers.\n\n\n\n\n\n\nCromwell‚Äôs Rule\n\n\n\n\n\nThis general preference for more liberal, less restrictive priors is also known as Cromwell‚Äôs Rule. The Cromwell in question is Oliver Cromwell, an English general who led a campaign against the Scottish army in 1650 and, in a letter to the Church of Scotland, wrote, ‚ÄúI beseech you, in the bowels of Christ, consider it possible that you are mistaken‚Äù (Jackman, 2009: 18). Lindley (1985), who named the rule, did so because wider priors allow us this possibility.\nReferences:\nJackman, S. (2009). Bayesian analysis for the social sciences. London: John Wiley & Sons, Ltd.\nLindley, D. V. (1985). Making decisions. 2nd ed.¬†London: John Wiley & Sons, Ltd.\n\n\n\nLet‚Äôs walk through how to come up with weakly regularising priors for \\(\\alpha\\) and the \\(\\beta\\)s in the model above.\n\nConsider order of magnitude/model space\nTLDR: Your priors must match the order of magnitude of the space the model is fit in.\nIf the model‚Äôs coefficient estimates are interpretable using the same units as the outcome (e.g., if you‚Äôre using a Gaussian likelihood), then we can say that the model is ‚Äúfit in the outcome space‚Äù. If your model is fit in the outcome space, then you need to think about what order of magnitude our outcome variable has.\n\nFor example, if your outcome is raw reaction time, then your effects might be in the hundreds (of milliseconds).\nBut if your outcome is log reaction time, then your effects are probably in the single digits (of log units).\n\nA weak prior on the log scale is an incredibly restrictive prior on the millisecond scale. So the prior has to match the order of magnitude of the outcome.\nIf the model is not fit in the outcome space‚Äîi.e., if the outcome is transformed into a different space, and then the linear model is fit there‚Äîwe need to know what space the model is fit in. This is because the priors have to be on the transformed scale, not the outcome scale.\nIn our case, our model has a logit link function that transforms probabilities into log-odds, and then the linear expression is fit in log-odds space. That means our priors have to be on the log-odds scale.\n\n\nA prior for \\(\\alpha\\)\n\\(\\alpha\\) represents the intercept of our linear model. It is the outcome, in log-odds space, when all predictors equal zero. Because we‚Äôll code up our predictors with $$0.5 sum coding, our \\(\\alpha\\) represents the grand mean of the outcome.\nOur goal: to come up with a weakly regularising prior that allows the outcome‚Äôs grand mean to take on basically any value. And for the intercept in particular, it‚Äôs useful to consider how this prior translates to probability space.\nLet‚Äôs have a look at three different log-odds priors (in the left panels), and how they look once back-transformed into probability space (right panels). The higher the probability density, the more plausible the model will consider those values to be.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot code to generate those plots\n\n\n\n\n\nHere‚Äôs the function that generates these log-odds vs.¬†probability plots, if you want to have a play yourself:\n\nplot_logodds_to_prob <- function(logodds_data, distrib_str){\n  # logodds_data: vector of numbers in log-odds space\n  # distrib_str: a string description of the log-odds distribution (for plot title)\n  \n  facet_labels <- c(\n    log_odds = 'In log-odds space',\n    prob     = 'In probability space'\n  )\n  tibble(log_odds = logodds_data) %>%\n    mutate(prob = plogis(log_odds)) %>% \n    pivot_longer(cols=everything(), names_to = 'scale', values_to = 'sim') %>% \n    ggplot(aes(x=sim)) +\n    facet_wrap(~ scale, scales = 'free',\n               labeller = as_labeller(facet_labels)) +\n    geom_density(fill='grey', alpha=.5) +\n    labs(\n      title = paste(\n        distrib_str, \n        'in log-odds space and transformed to probability space'\n      ),\n      x = element_blank(),\n      y = 'Probability density'\n    ) +\n    theme(axis.ticks.y = element_blank(),\n          axis.text.y = element_blank(),\n          panel.grid = element_blank()) +\n    NULL\n}\n\n# Example usage:\nplot_logodds_to_prob(rnorm(100000, 0, 2), 'Normal(0, 2)')\n\n\n\n\nWhat looks like an unassuming normal distribution in log-odds space can get pretty wacky in probability space. If we used \\(\\alpha \\sim\\) Normal(0, 2), then the model would think that probabilities of success near 0 and 1 are more plausible than probabilities around 0.5. And if we used \\(\\alpha \\sim\\) Normal(0, 1), then the model would think that those same extreme probabilities are very implausible, compared to the more moderate values. What we want is something in between.\nWhen Normal(0, 1.5) is transformed into probability space, it yields a decently uniform distribution: basically any probability of success is a priori equally plausible.\nWe like this permissiveness, so this will be our prior for \\(\\alpha\\). Our model becomes:\n\\[\n\\begin{aligned}\n\\text{acc} & \\sim Bernoulli(\\theta) \\\\\nlogit(\\theta) & = \\alpha + (\\beta_1 \\cdot cond) + (\\beta_2 \\cdot sent) + (\\beta_3 \\cdot cond \\cdot sent)\\\\\n\\alpha & \\sim Normal(0, 1.5) \\\\\n\\beta_1 & \\sim \\text{something} \\\\\n\\beta_2 & \\sim \\text{something} \\\\\n\\beta_3 & \\sim \\text{something} \\\\\n\\end{aligned}\n\\]\n\n\nA prior for the \\(\\beta\\)s\nThe \\(\\beta\\)s represent the effects of interest‚Äîthe main effects of \\(cond\\) and \\(sent\\), and the effect of their interaction‚Äîin log-odds space. Note: I generally use the same weakly regularising prior for all the \\(\\beta\\)s, especially if they‚Äôre all on the same scale or coded the same way.\nWe found a decent prior for the intercept, \\(\\alpha\\), by considering the mapping from log-odds to probability space. There‚Äôs no direct mapping for the \\(\\beta\\)s in the same way. This is because the \\(\\beta\\)s are added to the intercept in log-odds space; they never surface directly into the outcome space themselves, so it doesn‚Äôt make sense to think of them in terms of probabilities.\nThat \\(\\beta\\)s don‚Äôt stand alone makes it a bit harder to reason about what priors are sensible. How do we get around this? Enter prior predictive checks.\n\nPrior predictive checks in brms\nThe basic idea is that we can capitalise on Bayesian models‚Äô capacity as generative models to check whether different priors generate sensible data.\nWe tried it out for ourselves yesterday. But luckily, brms automates this for us!\nWe‚Äôll set up models with a few different priors and use them to generate simulated data. Then we‚Äôll look at that simulated data and see whether it looks fair. For us, ‚Äúfair‚Äù means not too restricted‚Äîwe want permissive priors (see Cromwell‚Äôs Rule above). This process is called ‚Äúprior prediction‚Äù or ‚Äúdoing prior predictive checks‚Äù. (We‚Äôll also do ‚Äúposterior predictive checks‚Äù after fitting the model‚Äîstay tuned.)\nHere‚Äôs the basic template for how to set up a prior predictive model in brms using the function brm().\nmymodel <- brm(myoutcome ~ mypredictor + (1 | mygroup),\n               data = mydata,\n               family = bernoulli(),\n               prior = c(\n                 prior(normal(0, 1.5), class = Intercept),\n                 prior(SOMETHING, class = b)\n               ),\n               backend = 'cmdstanr',\n               sample_prior = 'only')\n\nThe first argument is the model formula in lme4 syntax.\nThe data= argument tells brm() where to get the data.\nThe family= argument defines the model family (bernoulli(), gaussian(), etc.).\nThe prior= argument defines the priors that the model will use. If there is no prior= argument, the model will use the default priors.\nThe line backend = 'cmdstanr' argument specifies that the model will be fit using CmdStan.\nThe line sample_prior = 'only' is what makes this model into a prior predictive model: it ignores the data and uses only the priors to estimate the posteriors (basically just reproducing the priors). Removing this line will cause the model to take the data into account when estimating posteriors, and we‚Äôll do this when we properly fit the model.\n\nCopy the template above to fit a prior predictive model with the formula sentence_accepted ~ cond + sent + condsent and the very-wide prior normal(0, 10) for class = b (‚Äúb‚Äù stands for ‚Äúbeta‚Äù), using data from acc. Name the model priorpred10.\n\n\n\n\n\n\nBayesian hierarchical models\n\n\n\n\n\nIn an IRL version of this analysis, we‚Äôd of course also include group-level effects (what frequentists call ‚Äúrandom effects‚Äù). For ease of exposition, we‚Äôll skip those for now and just treat all data points as independent.\nIf you want to learn how to do hierarchical models in a Bayesian framework, see, e.g., here.\n\n\n\n\npriorpred10 <- ...\n\n\n\n\nThis model will estimate posteriors based on the priors only, ignoring all the data (essentially reproducing the priors). Now we‚Äôll use these posteriors to generate some new data and see whether it look reasonable.\nGenerating predictive data is such a common thing to do that brms comes with a useful function that helps us do it graphically: pp_check() (documentation here).\nUsing your model priorpred10, run the following code. Here‚Äôs what the arguments we pass to pp_check() mean:\n\ntype = 'stat' is one possible type of plot generated by pp_check(). It applies some summary statistic to the generated data.\nstat = mean says that our summary statistic is going to be the function mean().\nprefix = 'ppd' hides the observed data, which is shown by default, displaying only the predictive distributions (‚Äúppd‚Äù = ‚Äúprior predictive distribution‚Äù).\n\n\npp_check(priorpred10,      \n         type = 'stat',    \n         stat = mean,      \n         prefix = 'ppd') + \n  labs(x = 'theta',        # We can add the usual layers to the ggplot object!\n       title = 'Prior predictive distribution means with beta ~ Normal(0, 10)')\n\n\n\n\n\n\n\n\nThis plot is telling us that the mean probability of success (i.e., of accepting a sentence) in the prior predictive distributions is nearly always around 0.5, and it‚Äôs very unlikely to have probabilities of success above or below that. This doesn‚Äôt look so great‚Äîwe want a much broader and more even spread of means in the data that our model generates. Why? Because we want our model to be equally compatible with many different outcomes, not incredibly specific like this one is.\nSo let‚Äôs try a few more priors. Adapt the code above to fit a few more prior predictive models using these priors:\n\n\\(\\beta \\sim Normal(0, 5)\\)\n\\(\\beta \\sim Normal(0, 2)\\)\n\\(\\beta \\sim Normal(0, 1)\\)\n\\(\\beta \\sim Normal(0, 0.1)\\)\n\n(and any others you want to try!)\nFor each model, generate a summary histogram using pp_check(), and then have a think about the following questions:\n\nWhich of these priors produces the most even-looking distribution of outcome means?\nIs that the prior we‚Äôll want to use?\nWhat effect sizes do each of these priors consider reasonable?\n\nOnce you‚Äôve gathered your own thoughts, uncollapse this box to have a look at mine:\n\n\n\n\n\n\nPrior predictive distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen choosing between these priors, we need to balance an even spread of plausible values with a reasonable effect size.\nThe spread, perhaps counterintuitively, is most even with the narrowest prior, \\(Normal(0, 0.1)\\). But 95% of the plausible values in this prior fall within the range of about [‚Äì0.2, 0.2] log-odds‚Äîthat‚Äôs nothing!\nIf we move one step broader and consider \\(Normal(0, 1)\\), we still have a decently uniform spread of prior predictive means, and now 95% of plausible effect sizes are between [‚Äì2, 2] log-odds. That‚Äôs better.\nAnd the broader priors produce more skewed distributions, so we‚Äôll stick with \\(Normal(0, 1)\\). Our final model is:\n\\[\n\\begin{aligned}\n\\text{acc} & \\sim Bernoulli(\\theta) \\\\\nlogit(\\theta) & = \\alpha + (\\beta_1 \\cdot cond) + (\\beta_2 \\cdot sent) + (\\beta_3 \\cdot cond \\cdot sent)\\\\\n\\alpha & \\sim Normal(0, 1.5) \\\\\n\\beta_1 & \\sim Normal(0, 1) \\\\\n\\beta_2 & \\sim Normal(0, 1) \\\\\n\\beta_3 & \\sim Normal(0, 1) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "day2/build-model.html#summary-the-model-building-workflow",
    "href": "day2/build-model.html#summary-the-model-building-workflow",
    "title": "Building the model",
    "section": "Summary: The model-building workflow",
    "text": "Summary: The model-building workflow\n\nFind a likelihood suitable for the outcome data.\nIdentify the parameters required for the model you want to fit.\nUse prior predictive checks to find plausible priors for each of those parameters."
  },
  {
    "objectID": "day2/day2-recap.html",
    "href": "day2/day2-recap.html",
    "title": "Day 2 Recap",
    "section": "",
    "text": "What is the workflow for running a Bayesian analysis?\n\nFind a suitable likelihood.\nIdentify the parameters in the model.\nFind appropriate priors (using, e.g., prior predictive checks).\nFit the model.\nCheck convergence.\nCompare posterior predictive distributions to the true data.\nIf the model is off: Tweak it and begin again. Else: Report!\n\nHow do we fit a Bayesian model using brms?\n\nActually much like we would with lme4-based analyses!\nAnd with a little more patience.\n\nHow do we check that it‚Äôs a good model?\n\nTo check convergence, we use diagnostics like trace plots (üêõ) and Rhat values.\nTo see whether the model adequately captures the generative process behind the data, we compare data that the model generates to the true data we observed using posterior predictive checks.\n\nHow would we report the results in a publication?\n\nParameter estimates can be reported similarly to how you would report a frequentist model, bearing in mind that we are not rejecting any nulls, nor are we finding evidence for any effects."
  },
  {
    "objectID": "day2/fit-model.html",
    "href": "day2/fit-model.html",
    "title": "Fitting and checking the model",
    "section": "",
    "text": "On this page, we‚Äôll focus on:"
  },
  {
    "objectID": "day2/fit-model.html#our-model",
    "href": "day2/fit-model.html#our-model",
    "title": "Fitting and checking the model",
    "section": "Our model",
    "text": "Our model\nIn mathematical notation, the model we‚Äôll fit is the following.\n\\[\n\\begin{aligned}\n\\text{acc} & \\sim Bernoulli(\\theta) \\\\\nlogit(\\theta) & = \\alpha + (\\beta_1 \\cdot cond) + (\\beta_2 \\cdot sent) + (\\beta_3 \\cdot cond \\cdot sent)\\\\\n\\alpha & \\sim Normal(0, 1.5) \\\\\n\\beta_1 & \\sim Normal(0, 1) \\\\\n\\beta_2 & \\sim Normal(0, 1) \\\\\n\\beta_3 & \\sim Normal(0, 1) \\\\\n\\end{aligned}\n\\]\nIn brms notation, it looks like this:\n\nacc_fit <- brm(sentence_accepted ~ cond + sent + condsent,\n               data = acc,\n               family = bernoulli(),\n               prior = c(\n                 prior(normal(0, 1.5), class = Intercept),\n                 prior(normal(0, 1), class = b)\n               ),\n               backend = 'cmdstanr'\n)\n\nCopy this code into your notebook and run it to finally fit our model ü•≥ (It might take a minute or two, so be patient.)"
  },
  {
    "objectID": "day2/fit-model.html#fat-hairy-caterpillars",
    "href": "day2/fit-model.html#fat-hairy-caterpillars",
    "title": "Fitting and checking the model",
    "section": "‚ÄúFat hairy caterpillars‚Äù",
    "text": "‚ÄúFat hairy caterpillars‚Äù\nRun the following code to produce so-called ‚Äútrace plots‚Äù.\n\nmcmc_trace(acc_fit,\n           pars = c('b_Intercept', 'b_cond', 'b_sent', 'b_condsent'))\n\n\n\n\n\n\n\n\nTrace plots track where the four chains were as they traversed the posterior during sampling. The y axis represents the values of the posterior, and the x axis shows the sample indices (each chain drew 1,000 samples).\nIf the chains mixed well, then in the words of Shravan Vasishth, the trace plots should look like ‚Äúfat hairy caterpillars‚Äù. And these ones do!\nFor comparison, here‚Äôs an example of some that don‚Äôt so much.\n\n\n\n\n\n\n\n\n\nThese still mix a little, but they don‚Äôt overlap quite as densely as the previous ones.\nIn even worse scenarios, you might also sometimes see chains that get stuck in one part of the posterior and never mingle with the others: these show up on a trace plot as a straight horizontal line.\nBut as long as your trace plots look like fat hairy caterpillars, then that‚Äôs a good sign üêõ"
  },
  {
    "objectID": "day2/fit-model.html#rhat-1.00",
    "href": "day2/fit-model.html#rhat-1.00",
    "title": "Fitting and checking the model",
    "section": "Rhat = 1.00",
    "text": "Rhat = 1.00\nThe second thing to check is more quantitative. For each parameter, the model gives us a diagnostic measure called Rhat ([…ë…π h√¶t]). Rhat is a measure of how well the chains have mixed. It compares the estimates within chains and between chains, and if the chains have mixed well, then the result should be near 1. We consider the model to have converged if all Rhat values are equal to 1.00; even an Rhat of 1.01 should make you suspicious.\nIn the model summary, there is a column called Rhat. All values in this column should be equal to 1.00. (We‚Äôll look at the other values in this table on the next page!)\n\nsummary(acc_fit)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: sentence_accepted ~ cond + sent + condsent \n   Data: acc (Number of observations: 1476) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.32      0.06     0.20     0.43 1.00     4147     2917\ncond         -0.18      0.11    -0.41     0.04 1.00     4133     2921\nsent          1.84      0.12     1.62     2.08 1.00     4506     3221\ncondsent      0.22      0.12    -0.01     0.45 1.00     4438     3089\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf there is any Rhat value greater than 1.00, then it means the chains didn‚Äôt mix well enough, and we cannot trust the posterior estimates we‚Äôve been given. For reference, the model that yielded the bad trace plot above had Rhats = 1.04. But these ones look pretty good!\nIf you do ever fit a model that yields Rhat > 1.00, this page offers some solutions you could try.\nOK: our model acc_fit has converged. But there‚Äôs one more thing to check before we can trust its estimates: does it do a good job of generating data that looks like the data we observed? One way to find out‚Ä¶"
  },
  {
    "objectID": "day2/learningobj.html",
    "href": "day2/learningobj.html",
    "title": "Learning objectives for Day 2",
    "section": "",
    "text": "What is the workflow for running a Bayesian analysis?\nHow do we fit a Bayesian model using brms?\nHow do we check that it‚Äôs a good model?\nHow would we report the results in a publication?"
  },
  {
    "objectID": "day2/report-model.html",
    "href": "day2/report-model.html",
    "title": "Interpreting and reporting model results",
    "section": "",
    "text": "The posterior distributions estimated by the model are sampled using MCMC. This essentially means that each parameter has a posterior distribution consisting of many individual sampled values. Like with any variable, we can compute summary statistics over these distributions; that‚Äôs what summary() gives us.\n\nUnder Estimate, we get the mean of the posterior distribution of each parameter.\nUnder Est.Error, we get the standard deviation.\nUnder l-95% CI, we get the 2.5th quantile of the posterior (the lower bound of the 95% Credible Interval).\nUnder h-95% CI, we get the 97.5th quantile of the posterior (the upper bound of the 95% Credible Interval).\n\n\nsummary(acc_fit)\n\n Family: bernoulli \n  Links: mu = logit \nFormula: sentence_accepted ~ cond + sent + condsent \n   Data: acc (Number of observations: 1476) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.32      0.06     0.20     0.43 1.00     4147     2917\ncond         -0.18      0.11    -0.41     0.04 1.00     4133     2921\nsent          1.84      0.12     1.62     2.08 1.00     4506     3221\ncondsent      0.22      0.12    -0.01     0.45 1.00     4438     3089\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe‚Äôll go through these estimates now one by one and talk about how they can be understood. The parameter interpretations laid out here deviate in some important ways from the frequentist way of thinking. Before you read on, I encourage you to think about how you would interpret these numbers if they came out of, say, glm(), and compare that to the interpretations given here.\n\n\n\n\n\nmcmc_dens(acc_fit, pars = 'b_Intercept') +\n  labs(title = 'Posterior distribution of b_Intercept')\n\n\n\n\n\n\n\n\nThe estimated mean log-odds of accepting a sentence when all predictors = 0 is 0.32 log-odds (95% CrI: [0.20, 0.43]). This means that the posterior mean is 0.32 log-odds, and 95% of the posterior lies between 0.20 log-odds and 0.43 log-odds.\nBecause the posterior distribution is a distribution of belief over plausible parameter values, this means that the model thinks there‚Äôs a 95% probability that the intercept of this model is between 0.20 log-odds and 0.43 log-odds.\n\n\n\n\nmcmc_dens(acc_fit, pars = 'b_cond') +\n  labs(title = 'Posterior distribution of b_cond') +\n  geom_vline(xintercept = 0, linetype = 'dotted')\n\n\n\n\n\n\n\n\nThe mean effect of condition‚Äîi.e., the difference between production (+0.5) and comprehension (‚Äì0.5)‚Äîis ‚Äì0.18 log-odds (95% CrI: [‚Äì0.41, 0.04]).\nBecause the mean effect is negative, the model estimates that participants are, on average, more likely to accept sentences in the comprehension condition than the production condition. But, because the 95% CrI spans both positive and negative values, the model is not entirely certain about the direction of the effect. It believes that a small positive effect may also be consistent with this data.\n\n\n\n\nmcmc_dens(acc_fit, pars = 'b_sent') +\n  labs(title = 'Posterior distribution of b_sent')\n\n\n\n\n\n\n\n\nThe mean effect of sentence type‚Äîi.e., the difference between word order (+0.5) and case marking (‚Äì0.5)‚Äîis 1.84 log-odds (95% CrI: [1.62, 2.08]).\nBecause the entire 95% CrI is positive, the model is quite certain that this is a positive effect: participants are more likely to accept word order sentences than case-marking ones. This is also quite a large effect, compared to the others in the model.\n\n\n\n\nmcmc_dens(acc_fit, pars = 'b_condsent') +\n  labs(title = 'Posterior distribution of b_condsent') +\n  geom_vline(xintercept = 0, linetype = 'dotted')\n\n\n\n\n\n\n\n\nThe interaction between condition and sentence type is estimated at 0.22 log-odds (95% CrI: [‚Äì0.01, 0.45]).\nAgain, because the 95% CrI contains both positive and negative values, the model is not entirely certain about the direction of the interaction. It thinks that it likely is positive‚Äîwhich we can interpret as a larger difference between sentence types in the production condition than in the comprehension condition‚Äîbut it also is leaving open the possibility that the interaction may be small and negative.\n\n\n\n\nConsider the interpretations of b_cond and b_condsent. Both of their 95% Credible Intervals span both positive and negative values‚Äîin other words, the intervals contain zero.\nIf these were 95% confidence intervals and if we were using them to do frequentist hypothesis testing, the intervals containing zero would force us into one of the two possible outcomes: in this case, we could not reject the null. Having reached that binary decision, we‚Äôd emerge from the experiment not having learned much about any possible association of sentence acceptance with condition, or with the interaction between condition and sentence type.\nBut Bayesian models afford us much more interpretability. A 95% CrI containing zero doesn‚Äôt automatically nix the whole story. It just means that the model thinks that both negative and positive values are plausible values that the parameter in question could take on. And if it allocates more probability mass to the positive or the negative side of zero, that‚Äôs something we can report.\nTo their credit, some frequentist modellers also focus on effect size estimation and are moving away from hypothesis testing. But I‚Äôd argue that the Bayesian framework gives a nicely natural way of approaching this: in terms of allocation of belief to different parameter values.\n\n\n\n\n\n\nThe New Statistics\n\n\n\n\n\nI‚Äôm not the only one who thinks this way: here are a couple voices from the literature advocating the New Statistics, an approach that shifts the focus from hypothesis testing to effect size estimation.\nCumming, G. (2014). The New Statistics: Why and How. Psychological Science, 25(1), 7‚Äì29. https://doi.org/10.1177/0956797613504966\nKruschke, J. K., & Liddell, T. M. (2018). The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin & Review, 25(1), 178‚Äì206. https://doi.org/10.3758/s13423-016-1221-4"
  },
  {
    "objectID": "day2/report-model.html#to-report-the-results-of-this-model",
    "href": "day2/report-model.html#to-report-the-results-of-this-model",
    "title": "Interpreting and reporting model results",
    "section": "To report the results of this model",
    "text": "To report the results of this model\n\nIn tables\nUse the fixef() function to extract only the posterior summaries of the fixed effects. And then use xtable() from the library xtable to generate a LaTeX version of this table, ready (with a bit of tidying) for your next paper.\n\nlibrary(xtable)\nxtable(fixef(acc_fit))\n\n% latex table generated in R 4.3.0 by xtable 1.8-4 package\n% Wed May 10 13:41:17 2023\n\\begin{table}[ht]\n\\centering\n\\begin{tabular}{rrrrr}\n  \\hline\n & Estimate & Est.Error & Q2.5 & Q97.5 \\\\ \n  \\hline\nIntercept & 0.32 & 0.06 & 0.20 & 0.43 \\\\ \n  cond & -0.18 & 0.11 & -0.41 & 0.04 \\\\ \n  sent & 1.84 & 0.12 & 1.62 & 2.08 \\\\ \n  condsent & 0.22 & 0.12 & -0.01 & 0.45 \\\\ \n   \\hline\n\\end{tabular}\n\\end{table}\n\n\n\n\nIn plots\nIn the bayesplot package, you can find plots of all kinds designed for Bayesian models. mcmc_trace() and mcmc_dens() were two of them, and mcmc_areas() is another:\n\nmcmc_areas(acc_fit, \n           pars = c('b_Intercept', 'b_cond', 'b_sent', 'b_condsent'),\n           prob = 0.95,\n           stat = mean) +\n  geom_vline(xintercept = 0, linetype = 'dotted') +\n  labs(x = 'Log-odds')\n\n\n\n\n\n\n\n\nThis plot nicely visualises the posterior distributions in relation to one another, and I‚Äôve customised it to show the mean as a vertical line, and the 95% CrI as the shaded region. It can be customised this way because all bayesplot plots are ggplot objects.\n\n\nIn prose\nFinally, if you were going to write about this model in your paper, here‚Äôs how you might report it. For the sake of illustration, this example goes through every parameter‚Äîin reality, you‚Äôd probably focus mainly on the ones that are most relevant to your hypothesis (here, the interaction between condition and sentence type).\n\nWe fit a Bayesian linear model with a Bernoulli likelihood, predicting sentence acceptance as a function of condition, sentence type, and their interaction. The model used weakly regularising priors that we selected based on prior predictive checks.\nThe model estimates a likely-negative effect of condition (\\(\\beta\\) = ‚Äì0.18 log-odds, 95% CrI [‚Äì0.41, 0.04]), meaning that participants in the production condition are probably less likely to accept sentences than are participants in the comprehension condition. However, the 95% CrI contains positive values as well, indicating some uncertainty about the direction of the effect.\nThere is more certainty in the estimate of the much larger effect of sentence type (\\(\\beta\\) = 1.84 log-odds, 95% CrI [‚Äì1.62, 2.08]); the model is sure that word order sentences are much more likely to be accepted than case-marking sentences are.\nFinally, we hypothesised that we would observe a negative interaction, with a larger difference between sentence types in the comprehension condition than the production condition. However, the coefficient that the model estimates is almost entirely positive (\\(\\beta\\) = 0.22, 95% CrI: [‚Äì0.01, 0.45]), giving much more credibility to an interaction going in the opposite direction to the one we hypothesised.\n\nNotice that nowhere are we talking about ‚Äúevidence‚Äù or ‚Äúsignificance‚Äù or ‚Äúrejecting the null‚Äù. We‚Äôre also not focused on whether or not the 95% CrIs contain zero. Rather, we‚Äôre looking at the range of estimates that the 95% CrIs contain, and observing the extent to which this is consistent with our hypotheses.\n(If you do want to do Bayesian hypothesis testing and actually find ‚Äúevidence‚Äù for one model over another, you can do this using Bayes factors.)"
  },
  {
    "objectID": "day2/report-model.html#take-home-message-about-interpreting-bayesian-models",
    "href": "day2/report-model.html#take-home-message-about-interpreting-bayesian-models",
    "title": "Interpreting and reporting model results",
    "section": "Take-home message about interpreting Bayesian models",
    "text": "Take-home message about interpreting Bayesian models\nThe most that this Bayesian inferential model can do is allocate its belief over a range of possible effects, of possible parameter values; we are not finding evidence for or against any effects or hypotheses. This is a crucial thing to know and understand, and it might feel uncomfortable and nebulous at first. But the payoff to dealing with this uncertainty is a model that gives you much better interpretability."
  },
  {
    "objectID": "day2/setup.html",
    "href": "day2/setup.html",
    "title": "Getting set up",
    "section": "",
    "text": "I‚Äôve created a pre-loaded directory structure for this workshop that you can download. (If you‚Äôre having internet issues, ask me for the version on USB.)\n\nRight-click on this link and download the zip file (3.1 MB).\nUnzip it and relocate the directory to wherever you want on your computer.\nTo open the R project, click on the file bayes_workshop.Rproj (you might not see the .Rproj part‚Äîthen just click on the thing that says bayes_workshop). This will open the R project within RStudio; you‚Äôll be able to tell you‚Äôre in the R project because at the top right of your RStudio window, you‚Äôll see its name:\n\n\n\n\n\n\nIn the Files pane at the bottom right of RStudio, click on code/ and open the file called workshop.Rmd.\n\nR projects can be very helpful for creating reproducible analyses. For one, they let you set RStudio‚Äôs working directory to the project directory (i.e., the directory where the .Rproj file lives), and this makes reading in data that also lives within the project directory very easy.\nYou can see this at work in the R notebook I‚Äôve prepared."
  },
  {
    "objectID": "day2/setup.html#open-workshop.rmd",
    "href": "day2/setup.html#open-workshop.rmd",
    "title": "Getting set up",
    "section": "Open workshop.Rmd",
    "text": "Open workshop.Rmd\nworkshop.Rmd is where you‚Äôll build up the code to fit your Bayesian model.\nFirst up, make sure that the code that‚Äôs already in workshop.Rmd runs as it should. Execute all the code chunks in this file.\nIf everything is running correctly, you should see (1) no errors, (2) six lines of data, and (3) a plot that looks like this:\n\n\n\n\n\n\n\n\n\nIf you do, then you‚Äôre good to go! If not, let me know and we can troubleshoot."
  },
  {
    "objectID": "day2/setup.html#the-data-well-use",
    "href": "day2/setup.html#the-data-well-use",
    "title": "Getting set up",
    "section": "The data we‚Äôll use",
    "text": "The data we‚Äôll use\nThis data is a subset of the full dataset from a recent study run by Aislinn Keogh and Elizabeth Pankratz. We wanted to know whether the type of experimental task that participants do (a comprehension task vs.¬†a production task) can affect whether participants learn a word order grammar or a case marking grammar for an artificial language where both analyses are possible. (For more detail on the project, come see our virtual poster at CogSci this summer and stay tuned for our preprint üòâ)\nWe‚Äôll look at data from this experiment‚Äôs judgement phase, where participants are shown novel sentences that are constructed using either the word-order grammar or the case-marking grammar‚Äîsentences that are not compatible with both analyses. If participants learned one grammar, they should accept sentences constructed with that grammar and reject the others. In particular, we wanted to know whether participants in the production group would be more likely to accept case-marking sentences than participants in the comprehension group.\nThe plot you see in workshop.Rmd shows that this doesn‚Äôt seem to be the case‚Äîin fact, production participants reject case-marking sentences even more than comprehension participants do!"
  },
  {
    "objectID": "day2/setup.html#prepare-the-data",
    "href": "day2/setup.html#prepare-the-data",
    "title": "Getting set up",
    "section": "Prepare the data",
    "text": "Prepare the data\nWe are going to use \\(\\pm\\) 0.5 sum coding for the condition variable (comprehension as ‚Äì0.5; production as +0.5) and for the sentence type variable (case marking as ‚Äì0.5; word order as +0.5). Our hypothesis concerns the interaction between these predictors, so we‚Äôll include an interaction term as well. An interaction is the product of the two interacting variables, and we‚Äôll scale that by two so that the interaction also takes the values \\(\\pm\\) 0.5.\nTo set up the predictors we‚Äôll use, copy and run the following code in your R notebook:\n\nacc <- acc %>% \n  mutate(\n    cond = ifelse(condition == 'Comprehension', -0.5, 0.5),\n    sent = ifelse(sentence_type == 'case_marking', -0.5, 0.5),\n    condsent = cond * sent * 2\n  )\n\n(There are, of course, ways to set contrasts without creating whole new columns, e.g., using contr.sum(2)/2, but I like this way because it‚Äôs super explicit about what information the model‚Äôs going to use.)\nNext, let‚Äôs have a look at how to model this data using brms."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A crash course in Bayesian inference",
    "section": "",
    "text": "What you can expect from this workshop:\nIntended audience:"
  },
  {
    "objectID": "index.html#pre-workshop-set-up",
    "href": "index.html#pre-workshop-set-up",
    "title": "A crash course in Bayesian inference",
    "section": "Pre-workshop set-up",
    "text": "Pre-workshop set-up\nTo be able to run the code we‚Äôll use, please follow the set-up instructions here, and make sure everything is running before the workshop begins.\nTo check whether CmdStan has been correctly installed, you can run the following code in RStudio‚Äôs console:\n\nlibrary(cmdstanr)\nfile <- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod  <- cmdstan_model(file)\n\nIf running this code produces no errors and you end up with an object mod in your Environment pane, then you‚Äôre good to go!\nAlso make sure the following packages are installed:\n\ntidyverse\nbrms\nbayesplot\nknitr\n\n\nThanks to n loewen for help coming up with the workshop name :)\nCopyright Elizabeth Pankratz 2023."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Elizabeth‚Äôs recommended resources",
    "section": "",
    "text": "For total newcomers to stats in R:\n\nMaterials from Stefano Coretta‚Äôs Statistical and Quantitative Methods course from this past winter (partly developed by yours truly!).\n\nFor people seeking intuitions and lots of detail presented approachably:\n\nStatistical Rethinking by Richard McElreath.\n\nThis book doesn‚Äôt use brms, but here‚Äôs a translation into brms + tidyverse by A Solomon Kurz.\n\nMcElreath has a series of free lectures too.\n\nFor people seeking more formalisation and math:\n\nAn Introduction to Bayesian Data Analysis for Cognitive Science by Bruno Nicenboim, Daniel Schad, and Shravan Vasishth.\n\nThis book is my stats bible ‚ô•Ô∏è\n\n\nAnd work I don‚Äôt have personal experience with but that others say is helpful:\n\nA Student‚Äôs Guide to Bayesian Statistics by Ben Lambert"
  }
]